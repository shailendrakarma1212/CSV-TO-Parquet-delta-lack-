{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3b1c6f-c6e0-4f5f-829c-e2b3ca71e73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 CSV to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a08c6db-ad24-4974-a1b7-4def6be40215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the source CSV path\n",
    "csv_file_path = \"/Volumes/workspace/default/my_file/product_details.csv\"\n",
    "\n",
    "parquet_path = \"/Volumes/workspace/default/my_file/1output_parquet/product_details_parquet\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df_csv1 = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(csv_file_path)\n",
    "\n",
    "# Write the DataFrame to Parquet format, updating existing data\n",
    "df_csv1.write.format(\"parquet\").mode(\"overwrite\").save(parquet_path)\n",
    "\n",
    "# Verify by reading from the Parquet files\n",
    "df_parquet = spark.read.format(\"parquet\").load(parquet_path)\n",
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0267a6e4-a3ab-4002-9a61-0d140221572e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "e"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93bfc020-ce51-4b28-a60b-b51a6bfc184c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2 csv to Delta lakc file convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d03753-dbb9-4156-85d6-68f705cc5bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_file_path = \"/Volumes/workspace/default/my_file/penguins.csv\"\n",
    "\n",
    "\n",
    "delta_table_path = \"/Volumes/workspace/default/my_file/2output_delta_lake/penguins_delta\"\n",
    "\n",
    "\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(csv_file_path)\n",
    "\n",
    "# df_csv.show(5)\n",
    "\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "print(f\"CSV data converted to Delta Lake at: {delta_table_path}\")\n",
    "\n",
    "# Verify dalta file\n",
    "df_delta = spark.read.format(\"delta\").load(delta_table_path) # or spark.table(delta_table_name)\n",
    "df_delta.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f673007f-316f-4394-9c61-8fdd63ce745b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c69af6-3360-418d-abae-0c17cf661e3c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":520},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754378168133}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/workspace/default/my_file/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99377fdf-f7f4-4bda-b065-d8cf52a1ab77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/workspace/default/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8d9149-ddd1-4858-a868-11dcee8ca463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) CSV TO (Parquet & delta lack)2025-07-30 14:16:29",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
